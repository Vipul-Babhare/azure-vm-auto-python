# # # import streamlit as st
# # # import requests
# # # import time
# # # import concurrent.futures
# # # import pandas as pd
# # # import numpy as np
# # # import io
# # # import json
# # # import matplotlib.pyplot as plt
# # # import seaborn as sns
# # # from typing import Dict, List, Any
# # # import hashlib
# # # import os

# # # # --- Authentication Config ---
# # # VALID_CREDENTIALS = {
# # #     "admin": "password123",
# # #     "tester": "test2024", 
# # #     "user": "mypassword"
# # # }

# # # # --- Authentication Functions ---
# # # def hash_password(password):
# # #     return hashlib.sha256(password.encode()).hexdigest()

# # # def check_credentials(username, password):
# # #     return username in VALID_CREDENTIALS and VALID_CREDENTIALS[username] == password

# # # def login_form():
# # #     st.title("üîê Rainfall Model Dashboard - Login")
# # #     st.markdown("Please enter your credentials to access the testing dashboard.")
    
# # #     with st.form("login_form"):
# # #         col1, col2, col3 = st.columns([1, 2, 1])
# # #         with col2:
# # #             st.markdown("### Login")
# # #             username = st.text_input("Username", placeholder="Enter your username")
# # #             password = st.text_input("Password", type="password", placeholder="Enter your password")
# # #             submitted = st.form_submit_button("üöÄ Login", use_container_width=True)
# # #             if submitted:
# # #                 if check_credentials(username, password):
# # #                     st.session_state["authenticated"] = True
# # #                     st.session_state["username"] = username
# # #                     st.success("‚úÖ Login successful! Redirecting to dashboard...")
# # #                     time.sleep(1)
# # #                     st.rerun()
# # #                 else:
# # #                     st.error("‚ùå Invalid username or password. Please try again.")

# # # def logout():
# # #     st.session_state["authenticated"] = False
# # #     st.session_state["username"] = None
# # #     st.rerun()

# # # # --- Config ---
# # # MODEL_HOST = os.getenv("MODEL_HOST", "localhost")
# # # MODEL_PORT = os.getenv("MODEL_PORT", "8520")

# # # MODEL_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model:predict"
# # # PROFILING_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model/metadata"

# # # # --- Helper Functions ---
# # # def run_inference_with_detailed_timing(payload):
# # #     timings = {}
# # #     prep_start = time.time()
# # #     json_payload = json.dumps(payload)
# # #     timings['request_preparation'] = time.time() - prep_start
    
# # #     network_start = time.time()
# # #     try:
# # #         response = requests.post(MODEL_URL, json=payload)
# # #         timings['total_request'] = time.time() - network_start
# # #         process_start = time.time()
# # #         if response.status_code == 200:
# # #             result = response.json()
# # #             timings['response_processing'] = time.time() - process_start
# # #             return {"success": True, "result": result, "timings": timings, "status_code": response.status_code}
# # #         else:
# # #             return {"success": False, "error": f"HTTP {response.status_code}", "timings": timings, "status_code": response.status_code}
# # #     except Exception as e:
# # #         return {"success": False, "error": str(e), "timings": timings, "status_code": "Error"}

# # # def run_inference(payload):
# # #     start = time.time()
# # #     try:
# # #         response = requests.post(MODEL_URL, json=payload)
# # #         latency = time.time() - start
# # #         return {"latency": latency, "status_code": response.status_code,
# # #                 "response": response.json() if response.status_code == 200 else {}}
# # #     except Exception as e:
# # #         return {"latency": None, "status_code": "Error", "response": str(e)}

# # # def batch_payload(batch_size, feature_count=5):
# # #     return {"instances": [[round(np.random.random(), 3) for _ in range(feature_count)] for _ in range(batch_size)]}

# # # def throughput_test(batch_size, num_requests, max_workers=10):
# # #     payload = batch_payload(batch_size)
# # #     results = []
# # #     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
# # #         futures = [executor.submit(run_inference, payload) for _ in range(num_requests)]
# # #         for f in concurrent.futures.as_completed(futures):
# # #             results.append(f.result())
# # #     latencies = [r["latency"] for r in results if r["latency"] is not None]
# # #     throughput = len(latencies) / sum(latencies) if latencies else 0
# # #     stats = {
# # #         "batch_size": batch_size,
# # #         "num_requests": num_requests,
# # #         "avg_latency": np.mean(latencies) if latencies else None,
# # #         "p95_latency": np.percentile(latencies, 95) if latencies else None,
# # #         "max_latency": max(latencies) if latencies else None,
# # #         "throughput": throughput
# # #     }
# # #     return stats, results

# # # def profile_inference_breakdown(num_samples=10):
# # #     results = []
# # #     payload = batch_payload(1)
# # #     for _ in range(num_samples):
# # #         result = run_inference_with_detailed_timing(payload)
# # #         if result["success"]:
# # #             results.append(result["timings"])
# # #     if not results:
# # #         return None
# # #     df = pd.DataFrame(results)
# # #     stats = {
# # #         'component': list(df.columns),
# # #         'avg_time_ms': [df[col].mean() * 1000 for col in df.columns],
# # #         'std_time_ms': [df[col].std() * 1000 for col in df.columns],
# # #         'min_time_ms': [df[col].min() * 1000 for col in df.columns],
# # #         'max_time_ms': [df[col].max() * 1000 for col in df.columns]
# # #     }
# # #     return pd.DataFrame(stats), df

# # # # --- Tabs (unchanged except they use MODEL_URL dynamically) ---
# # # # [Keep all your existing tab functions here without IP changes because MODEL_URL is now dynamic]

# # # # --- Main Dashboard ---
# # # def main_dashboard():
# # #     col1, col2 = st.columns([3, 1])
# # #     with col1:
# # #         st.title("üîç Rainfall Model Testing Dashboard")
# # #         st.markdown(f"Test and benchmark the deployed TensorFlow model at `{MODEL_HOST}:{MODEL_PORT}`")
# # #     with col2:
# # #         st.markdown(f"**Welcome, {st.session_state.get('username', 'User')}!**")
# # #         if st.button("üö™ Logout", use_container_width=True):
# # #             logout()

# # #     tab1, tab2, tab3, tab4, tab5 = st.tabs([
# # #         "üß™ Smoke Test", 
# # #         "‚ö° Performance", 
# # #         "üìä Batch Analysis", 
# # #         "üîç Advanced Profiling", 
# # #         "üì• Reports"
# # #     ])
    
# # #     with tab1:
# # #         smoke_test_tab()
# # #     with tab2:
# # #         performance_test_tab()
# # #     with tab3:
# # #         batch_comparison_tab()
# # #     with tab4:
# # #         profiling_tab()
# # #     with tab5:
# # #         reports_tab()

# # #     st.markdown("---")
# # #     st.caption("Built for TensorFlow Serving Inference Analysis üöÄ")

# # # # --- Main ---
# # # def main():
# # #     if "authenticated" not in st.session_state:
# # #         st.session_state["authenticated"] = False
# # #     if "username" not in st.session_state:
# # #         st.session_state["username"] = None
# # #     if not st.session_state["authenticated"]:
# # #         login_form()
# # #     else:
# # #         main_dashboard()

# # # if __name__ == "__main__":
# # #     st.set_page_config(
# # #         page_title="Rainfall Model Testing Dashboard",
# # #         page_icon="üîç",
# # #         layout="wide",
# # #         initial_sidebar_state="collapsed"
# # #     )
# # #     st.markdown("""
# # #     <style>
# # #     .main > div { padding-top: 1rem; }
# # #     .stButton > button { width: 100%; }
# # #     .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p { font-size: 16px; }
# # #     .stTabs [data-baseweb="tab"] { height: 50px; }
# # #     </style>
# # #     """, unsafe_allow_html=True)
# # #     main()





# # import streamlit as st
# # import requests
# # import time
# # import concurrent.futures
# # import pandas as pd
# # import numpy as np
# # import io
# # import json
# # import matplotlib.pyplot as plt
# # import seaborn as sns
# # from typing import Dict, List, Any
# # import hashlib
# # import os

# # # --- Authentication Config ---
# # VALID_CREDENTIALS = {
# #     "admin": "password123",
# #     "tester": "test2024", 
# #     "user": "mypassword"
# # }

# # # --- Authentication Functions ---
# # def hash_password(password):
# #     return hashlib.sha256(password.encode()).hexdigest()

# # def check_credentials(username, password):
# #     return username in VALID_CREDENTIALS and VALID_CREDENTIALS[username] == password

# # def login_form():
# #     st.title("üîê Rainfall Model Dashboard - Login")
# #     st.markdown("Please enter your credentials to access the testing dashboard.")
    
# #     with st.form("login_form"):
# #         col1, col2, col3 = st.columns([1, 2, 1])
# #         with col2:
# #             st.markdown("### Login")
# #             username = st.text_input("Username", placeholder="Enter your username")
# #             password = st.text_input("Password", type="password", placeholder="Enter your password")
# #             submitted = st.form_submit_button("üöÄ Login", use_container_width=True)
# #             if submitted:
# #                 if check_credentials(username, password):
# #                     st.session_state["authenticated"] = True
# #                     st.session_state["username"] = username
# #                     st.success("‚úÖ Login successful! Redirecting to dashboard...")
# #                     time.sleep(1)
# #                     st.rerun()
# #                 else:
# #                     st.error("‚ùå Invalid username or password. Please try again.")

# # def logout():
# #     st.session_state["authenticated"] = False
# #     st.session_state["username"] = None
# #     st.rerun()

# # # --- Config ---
# # MODEL_HOST = os.getenv("MODEL_HOST", "localhost")
# # MODEL_PORT = os.getenv("MODEL_PORT", "8520")

# # MODEL_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model:predict"
# # PROFILING_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model/metadata"

# # # --- Helper Functions ---
# # def run_inference_with_detailed_timing(payload):
# #     timings = {}
# #     prep_start = time.time()
# #     json_payload = json.dumps(payload)
# #     timings['request_preparation'] = time.time() - prep_start
    
# #     network_start = time.time()
# #     try:
# #         response = requests.post(MODEL_URL, json=payload)
# #         timings['total_request'] = time.time() - network_start
# #         process_start = time.time()
# #         if response.status_code == 200:
# #             result = response.json()
# #             timings['response_processing'] = time.time() - process_start
# #             return {"success": True, "result": result, "timings": timings, "status_code": response.status_code}
# #         else:
# #             return {"success": False, "error": f"HTTP {response.status_code}", "timings": timings, "status_code": response.status_code}
# #     except Exception as e:
# #         return {"success": False, "error": str(e), "timings": timings, "status_code": "Error"}

# # def run_inference(payload):
# #     start = time.time()
# #     try:
# #         response = requests.post(MODEL_URL, json=payload)
# #         latency = time.time() - start
# #         return {"latency": latency, "status_code": response.status_code,
# #                 "response": response.json() if response.status_code == 200 else {}}
# #     except Exception as e:
# #         return {"latency": None, "status_code": "Error", "response": str(e)}

# # def batch_payload(batch_size, feature_count=5):
# #     return {"instances": [[round(np.random.random(), 3) for _ in range(feature_count)] for _ in range(batch_size)]}

# # def throughput_test(batch_size, num_requests, max_workers=10):
# #     payload = batch_payload(batch_size)
# #     results = []
# #     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
# #         futures = [executor.submit(run_inference, payload) for _ in range(num_requests)]
# #         for f in concurrent.futures.as_completed(futures):
# #             results.append(f.result())
# #     latencies = [r["latency"] for r in results if r["latency"] is not None]
# #     throughput = len(latencies) / sum(latencies) if latencies else 0
# #     stats = {
# #         "batch_size": batch_size,
# #         "num_requests": num_requests,
# #         "avg_latency": np.mean(latencies) if latencies else None,
# #         "p95_latency": np.percentile(latencies, 95) if latencies else None,
# #         "max_latency": max(latencies) if latencies else None,
# #         "throughput": throughput
# #     }
# #     return stats, results

# # def profile_inference_breakdown(num_samples=10):
# #     results = []
# #     payload = batch_payload(1)
# #     for _ in range(num_samples):
# #         result = run_inference_with_detailed_timing(payload)
# #         if result["success"]:
# #             results.append(result["timings"])
# #     if not results:
# #         return None
# #     df = pd.DataFrame(results)
# #     stats = {
# #         'component': list(df.columns),
# #         'avg_time_ms': [df[col].mean() * 1000 for col in df.columns],
# #         'std_time_ms': [df[col].std() * 1000 for col in df.columns],
# #         'min_time_ms': [df[col].min() * 1000 for col in df.columns],
# #         'max_time_ms': [df[col].max() * 1000 for col in df.columns]
# #     }
# #     return pd.DataFrame(stats), df

# # # --- Placeholder Tab Functions ---
# # def smoke_test_tab():
# #     st.subheader("üß™ Smoke Test")
# #     st.info("Smoke test functionality coming soon.")

# # def performance_test_tab():
# #     st.subheader("‚ö° Performance Test")
# #     st.info("Performance testing functionality coming soon.")

# # def batch_comparison_tab():
# #     st.subheader("üìä Batch Analysis")
# #     st.info("Batch analysis functionality coming soon.")

# # def profiling_tab():
# #     st.subheader("üîç Advanced Profiling")
# #     st.info("Advanced profiling functionality coming soon.")

# # def reports_tab():
# #     st.subheader("üì• Reports")
# #     st.info("Reports functionality coming soon.")

# # # --- Main Dashboard ---
# # def main_dashboard():
# #     col1, col2 = st.columns([3, 1])
# #     with col1:
# #         st.title("üîç Rainfall Model Testing Dashboard")
# #         st.markdown(f"Test and benchmark the deployed TensorFlow model at `{MODEL_HOST}:{MODEL_PORT}`")
# #     with col2:
# #         st.markdown(f"**Welcome, {st.session_state.get('username', 'User')}!**")
# #         if st.button("üö™ Logout", use_container_width=True):
# #             logout()

# #     tab1, tab2, tab3, tab4, tab5 = st.tabs([
# #         "üß™ Smoke Test", 
# #         "‚ö° Performance", 
# #         "üìä Batch Analysis", 
# #         "üîç Advanced Profiling", 
# #         "üì• Reports"
# #     ])
    
# #     with tab1:
# #         smoke_test_tab()
# #     with tab2:
# #         performance_test_tab()
# #     with tab3:
# #         batch_comparison_tab()
# #     with tab4:
# #         profiling_tab()
# #     with tab5:
# #         reports_tab()

# #     st.markdown("---")
# #     st.caption("Built for TensorFlow Serving Inference Analysis üöÄ")

# # # --- Main ---
# # def main():
# #     if "authenticated" not in st.session_state:
# #         st.session_state["authenticated"] = False
# #     if "username" not in st.session_state:
# #         st.session_state["username"] = None
# #     if not st.session_state["authenticated"]:
# #         login_form()
# #     else:
# #         main_dashboard()

# # if __name__ == "__main__":
# #     st.set_page_config(
# #         page_title="Rainfall Model Testing Dashboard",
# #         page_icon="üîç",
# #         layout="wide",
# #         initial_sidebar_state="collapsed"
# #     )
# #     st.markdown("""
# #     <style>
# #     .main > div { padding-top: 1rem; }
# #     .stButton > button { width: 100%; }
# #     .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p { font-size: 16px; }
# #     .stTabs [data-baseweb="tab"] { height: 50px; }
# #     </style>
# #     """, unsafe_allow_html=True)
# #     main() 



# import streamlit as st
# import requests
# import time
# import concurrent.futures
# import pandas as pd
# import numpy as np
# import io
# import json
# import matplotlib.pyplot as plt
# import seaborn as sns
# from typing import Dict, List, Any, Optional, Tuple, Union
# import hashlib
# import os
 
# # --- Authentication Config ---
# VALID_CREDENTIALS = {
#     "admin": "password123",
#     "tester": "test2024",
#     "user": "mypassword"
# }
 
# # --- Authentication Functions ---
# def hash_password(password: str) -> str:
#     """Hash password using SHA256"""
#     return hashlib.sha256(password.encode()).hexdigest()
 
# def check_credentials(username: str, password: str) -> bool:
#     """Verify username and password against valid credentials"""
#     return username in VALID_CREDENTIALS and VALID_CREDENTIALS[username] == password
 
# def login_form():
#     """Display login form"""
#     st.title("üîê Rainfall Model Dashboard - Login")
#     st.markdown("Please enter your credentials to access the testing dashboard.")
 
#     with st.form("login_form"):
#         col1, col2, col3 = st.columns([1, 2, 1])
#         with col2:
#             st.markdown("### Login")
#             username = st.text_input("Username", placeholder="Enter your username")
#             password = st.text_input("Password", type="password", placeholder="Enter your password")
#             submitted = st.form_submit_button("üöÄ Login", use_container_width=True)
#             if submitted:
#                 if check_credentials(username, password):
#                     st.session_state["authenticated"] = True
#                     st.session_state["username"] = username
#                     st.success("‚úÖ Login successful! Redirecting to dashboard...")
#                     time.sleep(1)
#                     st.rerun()
#                 else:
#                     st.error("‚ùå Invalid username or password. Please try again.")
 
# def logout():
#     """Clear authentication session and redirect to login"""
#     st.session_state["authenticated"] = False
#     st.session_state["username"] = None
#     st.rerun()
 
# # --- Config ---
# MODEL_HOST = os.getenv("MODEL_HOST", "localhost")
# MODEL_PORT = os.getenv("MODEL_PORT", "8520")
 
# MODEL_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model:predict"
# PROFILING_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model/metadata"
 
# # --- Helper Functions ---
# def run_inference_with_detailed_timing(payload: Dict[str, Any]) -> Dict[str, Any]:
#     """Run inference with detailed timing breakdown"""
#     timings = {}
#     prep_start = time.time()
#     # Removed unused json_payload variable
#     timings['request_preparation'] = time.time() - prep_start
 
#     network_start = time.time()
#     try:
#         response = requests.post(MODEL_URL, json=payload)
#         timings['total_request'] = time.time() - network_start
#         process_start = time.time()
#         if response.status_code == 200:
#             result = response.json()
#             timings['response_processing'] = time.time() - process_start
#             return {"success": True, "result": result, "timings": timings, "status_code": response.status_code}
#         else:
#             return {"success": False, "error": f"HTTP {response.status_code}", "timings": timings, "status_code": response.status_code}
#     except Exception as e:
#         return {"success": False, "error": str(e), "timings": timings, "status_code": "Error"}
 
# def run_inference(payload: Dict[str, Any]) -> Dict[str, Any]:
#     """Run single inference request"""
#     start = time.time()
#     try:
#         response = requests.post(MODEL_URL, json=payload)
#         latency = time.time() - start
#         return {"latency": latency, "status_code": response.status_code,
#                 "response": response.json() if response.status_code == 200 else {}}
#     except Exception as e:
#         return {"latency": None, "status_code": "Error", "response": str(e)}
 
# def batch_payload(batch_size: int, feature_count: int = 5) -> Dict[str, List[List[float]]]:
#     """Generate batch payload for inference"""
#     return {"instances": [[round(np.random.random(), 3) for _ in range(feature_count)] for _ in range(batch_size)]}
 
# def throughput_test(batch_size: int, num_requests: int, max_workers: int = 10) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
#     """Run throughput test with multiple concurrent requests"""
#     payload = batch_payload(batch_size)
#     results = []
#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
#         futures = [executor.submit(run_inference, payload) for _ in range(num_requests)]
#         for f in concurrent.futures.as_completed(futures):
#             results.append(f.result())
   
#     latencies = [r["latency"] for r in results if r["latency"] is not None]
#     throughput = len(latencies) / sum(latencies) if latencies else 0
#     stats = {
#         "batch_size": batch_size,
#         "num_requests": num_requests,
#         "avg_latency": np.mean(latencies) if latencies else None,
#         "p95_latency": np.percentile(latencies, 95) if latencies else None,
#         "max_latency": max(latencies) if latencies else None,
#         "throughput": throughput
#     }
#     return stats, results
 
# def profile_inference_breakdown(num_samples: int = 10) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
#     """Profile inference timing breakdown - FIXED: Consistent return types"""
#     results = []
#     payload = batch_payload(1)
   
#     for _ in range(num_samples):
#         result = run_inference_with_detailed_timing(payload)
#         if result["success"]:
#             results.append(result["timings"])
   
#     # Always return a tuple for consistency
#     if not results:
#         return None, None
   
#     df = pd.DataFrame(results)
#     stats = {
#         'component': list(df.columns),
#         'avg_time_ms': [df[col].mean() * 1000 for col in df.columns],
#         'std_time_ms': [df[col].std() * 1000 for col in df.columns],
#         'min_time_ms': [df[col].min() * 1000 for col in df.columns],
#         'max_time_ms': [df[col].max() * 1000 for col in df.columns]
#     }
#     return pd.DataFrame(stats), df
 
# # --- Placeholder Tab Functions ---
# def smoke_test_tab():
#     """Smoke test tab implementation"""
#     st.subheader("üß™ Smoke Test")
   
#     col1, col2 = st.columns([2, 1])
#     with col1:
#         st.info("Test basic model connectivity and response validation")
#     with col2:
#         if st.button("üöÄ Run Smoke Test", use_container_width=True):
#             with st.spinner("Running smoke test..."):
#                 test_payload = batch_payload(1)
#                 result = run_inference_with_detailed_timing(test_payload)
               
#                 if result["success"]:
#                     st.success("‚úÖ Smoke test passed!")
#                     st.json(result["result"])
#                 else:
#                     st.error(f"‚ùå Smoke test failed: {result.get('error', 'Unknown error')}")
 
# def performance_test_tab():
#     """Performance test tab implementation"""
#     st.subheader("‚ö° Performance Test")
   
#     col1, col2, col3 = st.columns(3)
#     with col1:
#         batch_size = st.number_input("Batch Size", min_value=1, max_value=100, value=10)
#     with col2:
#         num_requests = st.number_input("Number of Requests", min_value=1, max_value=1000, value=50)
#     with col3:
#         max_workers = st.number_input("Concurrent Workers", min_value=1, max_value=50, value=10)
   
#     if st.button("üöÄ Run Performance Test", use_container_width=True):
#         with st.spinner("Running performance test..."):
#             stats, results = throughput_test(batch_size, num_requests, max_workers)
           
#             col1, col2, col3, col4 = st.columns(4)
#             with col1:
#                 st.metric("Avg Latency", f"{stats['avg_latency']:.3f}s" if stats['avg_latency'] else "N/A")
#             with col2:
#                 st.metric("P95 Latency", f"{stats['p95_latency']:.3f}s" if stats['p95_latency'] else "N/A")
#             with col3:
#                 st.metric("Max Latency", f"{stats['max_latency']:.3f}s" if stats['max_latency'] else "N/A")
#             with col4:
#                 st.metric("Throughput", f"{stats['throughput']:.2f} req/s")
 
# def batch_comparison_tab():
#     """Batch analysis tab implementation"""
#     st.subheader("üìä Batch Analysis")
   
#     st.info("Compare performance across different batch sizes")
   
#     batch_sizes = st.multiselect(
#         "Select batch sizes to compare",
#         options=[1, 5, 10, 20, 50, 100],
#         default=[1, 10, 50]
#     )
   
#     num_requests = st.slider("Requests per batch size", 1, 100, 20)
   
#     if st.button("üöÄ Run Batch Comparison", use_container_width=True) and batch_sizes:
#         comparison_results = []
#         progress_bar = st.progress(0)
       
#         for i, batch_size in enumerate(batch_sizes):
#             with st.spinner(f"Testing batch size {batch_size}..."):
#                 stats, _ = throughput_test(batch_size, num_requests, 5)
#                 comparison_results.append(stats)
#                 progress_bar.progress((i + 1) / len(batch_sizes))
       
#         # Display results
#         df_results = pd.DataFrame(comparison_results)
#         st.dataframe(df_results, use_container_width=True)
       
#         # Create visualization
#         if len(comparison_results) > 1:
#             fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
           
#             # Latency comparison
#             ax1.plot(df_results['batch_size'], df_results['avg_latency'], marker='o', label='Avg Latency')
#             ax1.plot(df_results['batch_size'], df_results['p95_latency'], marker='s', label='P95 Latency')
#             ax1.set_xlabel('Batch Size')
#             ax1.set_ylabel('Latency (s)')
#             ax1.set_title('Latency vs Batch Size')
#             ax1.legend()
#             ax1.grid(True, alpha=0.3)
           
#             # Throughput comparison
#             ax2.plot(df_results['batch_size'], df_results['throughput'], marker='o', color='green')
#             ax2.set_xlabel('Batch Size')
#             ax2.set_ylabel('Throughput (req/s)')
#             ax2.set_title('Throughput vs Batch Size')
#             ax2.grid(True, alpha=0.3)
           
#             st.pyplot(fig)
 
# def profiling_tab():
#     """Advanced profiling tab implementation"""
#     st.subheader("üîç Advanced Profiling")
   
#     col1, col2 = st.columns([2, 1])
#     with col1:
#         st.info("Analyze detailed timing breakdown of inference pipeline")
#         num_samples = st.slider("Number of samples", 5, 50, 20)
   
#     with col2:
#         if st.button("üöÄ Run Profiling", use_container_width=True):
#             with st.spinner("Profiling inference pipeline..."):
#                 stats_df, raw_df = profile_inference_breakdown(num_samples)
               
#                 if stats_df is not None and raw_df is not None:
#                     st.success("‚úÖ Profiling completed!")
                   
#                     # Display timing statistics
#                     st.subheader("üìä Timing Statistics")
#                     st.dataframe(stats_df, use_container_width=True)
                   
#                     # Create visualization
#                     fig, ax = plt.subplots(figsize=(10, 6))
#                     x_pos = np.arange(len(stats_df))
#                     bars = ax.bar(x_pos, stats_df['avg_time_ms'], yerr=stats_df['std_time_ms'], capsize=5)
#                     ax.set_xlabel('Pipeline Component')
#                     ax.set_ylabel('Time (ms)')
#                     ax.set_title('Inference Pipeline Timing Breakdown')
#                     ax.set_xticks(x_pos)
#                     ax.set_xticklabels(stats_df['component'], rotation=45)
#                     ax.grid(True, alpha=0.3)
                   
#                     # Add value labels on bars
#                     for i, bar in enumerate(bars):
#                         height = bar.get_height()
#                         ax.text(bar.get_x() + bar.get_width()/2., height + stats_df['std_time_ms'].iloc[i],
#                                f'{height:.2f}ms', ha='center', va='bottom')
                   
#                     plt.tight_layout()
#                     st.pyplot(fig)
                   
#                     # Show raw data
#                     with st.expander("üìã Raw Timing Data"):
#                         st.dataframe(raw_df, use_container_width=True)
#                 else:
#                     st.error("‚ùå Profiling failed - no successful requests")
 
# def reports_tab():
#     """Reports tab implementation"""
#     st.subheader("üì• Reports")
   
#     col1, col2 = st.columns(2)
#     with col1:
#         st.info("Export test results and generate reports")
#         report_type = st.selectbox(
#             "Select report type",
#             ["Performance Summary", "Batch Analysis", "Profiling Report", "Full Test Suite"]
#         )
   
#     with col2:
#         st.markdown("### Quick Actions")
#         if st.button("üìä Generate Sample Report", use_container_width=True):
#             # Generate a sample report
#             sample_data = {
#                 'Metric': ['Avg Latency', 'P95 Latency', 'Throughput', 'Success Rate'],
#                 'Value': ['0.125s', '0.234s', '45.2 req/s', '99.8%'],
#                 'Status': ['‚úÖ Good', '‚ö†Ô∏è Fair', '‚úÖ Good', '‚úÖ Excellent']
#             }
           
#             st.subheader(f"üìã {report_type}")
#             st.dataframe(pd.DataFrame(sample_data), use_container_width=True)
           
#             # Create download link for CSV
#             csv = pd.DataFrame(sample_data).to_csv(index=False)
#             st.download_button(
#                 label="üíæ Download CSV",
#                 data=csv,
#                 file_name=f"rainfall_model_{report_type.lower().replace(' ', '_')}.csv",
#                 mime="text/csv"
#             )
 
# # --- Main Dashboard ---
# def main_dashboard():
#     """Main dashboard with tabs"""
#     col1, col2 = st.columns([3, 1])
#     with col1:
#         st.title("üîç Rainfall Model Testing Dashboard")
#         st.markdown(f"Test and benchmark the deployed TensorFlow model at `{MODEL_HOST}:{MODEL_PORT}`")
#     with col2:
#         st.markdown(f"**Welcome, {st.session_state.get('username', 'User')}!**")
#         if st.button("üö™ Logout", use_container_width=True):
#             logout()
 
#     # Model status check
#     with st.container():
#         col1, col2, col3 = st.columns(3)
#         with col1:
#             try:
#                 test_result = run_inference(batch_payload(1))
#                 if test_result["status_code"] == 200:
#                     st.success("üü¢ Model Online")
#                 elif test_result["status_code"] == "Error":
#                     st.error("üî¥ TensorFlow model URL is unreachable/unavailable")
#                 else:
#                     st.error(f"üî¥ Model Error (HTTP {test_result['status_code']})")
#             except Exception as e:
#                 st.error("üî¥ TensorFlow model URL is unreachable/unavailable")
       
#         with col2:
#             st.info(f"üåê Host: {MODEL_HOST}:{MODEL_PORT}")
#         with col3:
#             st.info(f"üïí Last Check: {time.strftime('%H:%M:%S')}")
 
#     # Main tabs
#     tab1, tab2, tab3, tab4, tab5 = st.tabs([
#         "üß™ Smoke Test",
#         "‚ö° Performance",
#         "üìä Batch Analysis",
#         "üîç Advanced Profiling",
#         "üì• Reports"
#     ])
 
#     with tab1:
#         smoke_test_tab()
#     with tab2:
#         performance_test_tab()
#     with tab3:
#         batch_comparison_tab()
#     with tab4:
#         profiling_tab()
#     with tab5:
#         reports_tab()
 
#     st.markdown("---")
#     st.caption("Built for TensorFlow Serving Inference Analysis üöÄ")
 
# # --- Main ---
# def main():
#     """Main application entry point"""
#     # Initialize session state
#     if "authenticated" not in st.session_state:
#         st.session_state["authenticated"] = False
#     if "username" not in st.session_state:
#         st.session_state["username"] = None
   
#     # Show login or dashboard based on authentication
#     if not st.session_state["authenticated"]:
#         login_form()
#     else:
#         main_dashboard()
 
# if __name__ == "__main__":
#     # Configure Streamlit page
#     st.set_page_config(
#         page_title="Rainfall Model Testing Dashboard",
#         page_icon="üîç",
#         layout="wide",
#         initial_sidebar_state="collapsed"
#     )
   
#     # Custom CSS styling
#     st.markdown("""
#     <style>
#     .main > div { padding-top: 1rem; }
#     .stButton > button { width: 100%; }
#     .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p { font-size: 16px; }
#     .stTabs [data-baseweb="tab"] { height: 50px; }
#     .stMetric > label { font-size: 14px; }
#     .stSuccess, .stError, .stWarning, .stInfo { margin-top: 1rem; margin-bottom: 1rem; }
#     </style>
#     """, unsafe_allow_html=True)
   
#     main()


import streamlit as st
import requests
import time
import concurrent.futures
import pandas as pd
import numpy as np
import io
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple, Union
import hashlib
import os

# --- Authentication Config ---
VALID_CREDENTIALS = {
    "admin": "password123",
    "tester": "test2024", 
    "user": "mypassword"
}

# --- Authentication Functions ---
def hash_password(password: str) -> str:
    """Hash password using SHA256"""
    return hashlib.sha256(password.encode()).hexdigest()

def check_credentials(username: str, password: str) -> bool:
    """Verify username and password against valid credentials"""
    return username in VALID_CREDENTIALS and VALID_CREDENTIALS[username] == password

def login_form():
    """Display login form"""
    st.title("üîê Rainfall Model Dashboard - Login")
    st.markdown("Please enter your credentials to access the testing dashboard.")

    with st.form("login_form"):
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            st.markdown("### Login")
            username = st.text_input("Username", placeholder="Enter your username")
            password = st.text_input("Password", type="password", placeholder="Enter your password")
            submitted = st.form_submit_button("üöÄ Login", use_container_width=True)
            if submitted:
                if check_credentials(username, password):
                    st.session_state["authenticated"] = True
                    st.session_state["username"] = username
                    st.success("‚úÖ Login successful! Redirecting to dashboard...")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("‚ùå Invalid username or password. Please try again.")

def logout():
    """Clear authentication session and redirect to login"""
    st.session_state["authenticated"] = False
    st.session_state["username"] = None
    st.rerun()

# --- Config ---
def get_model_host():
    """Dynamically determine model host IP"""
    # First, try environment variable (for explicit override)
    model_host = os.getenv("MODEL_HOST")
    if model_host and model_host.strip():
        return model_host.strip()
    
    # Check if we're running in Azure VM
    try:
        import requests
        azure_metadata_url = os.getenv("AZURE_METADATA_URL", 
            "http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/publicIpAddress?api-version=2021-02-01&format=text")
        
        response = requests.get(azure_metadata_url, 
                              headers={"Metadata": "true"}, 
                              timeout=5)
        if response.status_code == 200:
            public_ip = response.text.strip()
            if public_ip and public_ip != "":
                # Test if model is accessible via localhost first (same VM)
                try:
                    test_response = requests.get(f"http://localhost:8520/v1/models", timeout=3)
                    if test_response.status_code in [200, 404]:  # 404 is OK, means server is responding
                        print(f"Model accessible via localhost, using localhost instead of {public_ip}")
                        return "localhost"
                except:
                    pass
                
                # If localhost doesn't work, use public IP
                print(f"Using public IP from Azure metadata: {public_ip}")
                return public_ip
    except Exception as e:
        print(f"Failed to get Azure metadata: {e}")
    
    # Fallback to localhost for local development
    return "localhost"

MODEL_HOST = get_model_host()
MODEL_PORT = os.getenv("MODEL_PORT", "8520")

MODEL_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model:predict"
PROFILING_URL = f"http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model/metadata"

# --- Helper Functions ---
def run_inference_with_detailed_timing(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Run inference with detailed timing breakdown"""
    timings = {}
    prep_start = time.time()
    # Removed unused json_payload variable
    timings['request_preparation'] = time.time() - prep_start

    network_start = time.time()
    try:
        response = requests.post(MODEL_URL, json=payload)
        timings['total_request'] = time.time() - network_start
        process_start = time.time()
        if response.status_code == 200:
            result = response.json()
            timings['response_processing'] = time.time() - process_start
            return {"success": True, "result": result, "timings": timings, "status_code": response.status_code}
        else:
            return {"success": False, "error": f"HTTP {response.status_code}", "timings": timings, "status_code": response.status_code}
    except Exception as e:
        return {"success": False, "error": str(e), "timings": timings, "status_code": "Error"}

def run_inference(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Run single inference request"""
    start = time.time()
    try:
        response = requests.post(MODEL_URL, json=payload)
        latency = time.time() - start
        return {"latency": latency, "status_code": response.status_code,
                "response": response.json() if response.status_code == 200 else {}}
    except Exception as e:
        return {"latency": None, "status_code": "Error", "response": str(e)}

def batch_payload(batch_size: int, feature_count: int = 5) -> Dict[str, List[List[float]]]:
    """Generate batch payload for inference"""
    return {"instances": [[round(np.random.random(), 3) for _ in range(feature_count)] for _ in range(batch_size)]}

def throughput_test(batch_size: int, num_requests: int, max_workers: int = 10) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
    """Run throughput test with multiple concurrent requests"""
    payload = batch_payload(batch_size)
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(run_inference, payload) for _ in range(num_requests)]
        for f in concurrent.futures.as_completed(futures):
            results.append(f.result())
    
    latencies = [r["latency"] for r in results if r["latency"] is not None]
    throughput = len(latencies) / sum(latencies) if latencies else 0
    stats = {
        "batch_size": batch_size,
        "num_requests": num_requests,
        "avg_latency": np.mean(latencies) if latencies else None,
        "p95_latency": np.percentile(latencies, 95) if latencies else None,
        "max_latency": max(latencies) if latencies else None,
        "throughput": throughput
    }
    return stats, results

def profile_inference_breakdown(num_samples: int = 10) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
    """Profile inference timing breakdown - FIXED: Consistent return types"""
    results = []
    payload = batch_payload(1)
    
    for _ in range(num_samples):
        result = run_inference_with_detailed_timing(payload)
        if result["success"]:
            results.append(result["timings"])
    
    # Always return a tuple for consistency
    if not results:
        return None, None
    
    df = pd.DataFrame(results)
    stats = {
        'component': list(df.columns),
        'avg_time_ms': [df[col].mean() * 1000 for col in df.columns],
        'std_time_ms': [df[col].std() * 1000 for col in df.columns],
        'min_time_ms': [df[col].min() * 1000 for col in df.columns],
        'max_time_ms': [df[col].max() * 1000 for col in df.columns]
    }
    return pd.DataFrame(stats), df

# --- Placeholder Tab Functions ---
def smoke_test_tab():
    """Smoke test tab implementation"""
    st.subheader("üß™ Smoke Test")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.info("Test basic model connectivity and response validation")
    with col2:
        if st.button("üöÄ Run Smoke Test", use_container_width=True):
            with st.spinner("Running smoke test..."):
                test_payload = batch_payload(1)
                result = run_inference_with_detailed_timing(test_payload)
                
                if result["success"]:
                    st.success("‚úÖ Smoke test passed!")
                    st.json(result["result"])
                else:
                    error_msg = result.get('error', 'Unknown error')
                    if "Connection refused" in error_msg or "Failed to establish" in error_msg:
                        st.error("‚ùå **TensorFlow model URL is unreachable/unavailable**")
                        st.warning(f"Please ensure your TensorFlow Serving model is running on `{MODEL_HOST}:{MODEL_PORT}`")
                        
                        with st.expander("üõ†Ô∏è Quick Troubleshooting Guide"):
                            st.markdown(f"""
                            **Steps to resolve:**
                            1. **Start TensorFlow Serving** with your rainfall model
                            2. **Verify the port** - Make sure it's running on port `{MODEL_PORT}`
                            3. **Check model name** - Ensure your model is named `rainfall_model`
                            4. **Test connection**: 
                               ```bash
                               curl http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model
                               ```
                            
                            **Example prediction test:**
                            ```bash
                            curl -X POST http://{MODEL_HOST}:{MODEL_PORT}/v1/models/rainfall_model:predict \\
                              -H "Content-Type: application/json" \\
                              -d '{{"instances": [[1, 45, 4, 30, 90]]}}'
                            ```
                            
                            **To change host/port in dashboard:**
                            ```bash
                            export MODEL_HOST=your_host
                            export MODEL_PORT=your_port
                            streamlit run your_app.py
                            ```
                            """)
                        
                        with st.expander("üîç Technical Details"):
                            st.code(error_msg)
                    elif result.get('status_code') and str(result['status_code']).startswith('4'):
                        st.error(f"‚ùå **Model endpoint error (HTTP {result['status_code']})**")
                        st.warning("Check if the model name 'rainfall_model' is correct")
                    elif result.get('status_code') and str(result['status_code']).startswith('5'):
                        st.error(f"‚ùå **TensorFlow Serving internal error (HTTP {result['status_code']})**")
                        st.warning("Check TensorFlow Serving logs for details")
                    else:
                        st.error("‚ùå **Smoke test failed**")
                        with st.expander("üîç Error Details"):
                            st.code(error_msg)

def performance_test_tab():
    """Performance test tab implementation"""
    st.subheader("‚ö° Performance Test")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        batch_size = st.number_input("Batch Size", min_value=1, max_value=100, value=10)
    with col2:
        num_requests = st.number_input("Number of Requests", min_value=1, max_value=1000, value=50)
    with col3:
        max_workers = st.number_input("Concurrent Workers", min_value=1, max_value=50, value=10)
    
    if st.button("üöÄ Run Performance Test", use_container_width=True):
        with st.spinner("Running performance test..."):
            stats, results = throughput_test(batch_size, num_requests, max_workers)
            
            # Check if any results were successful
            successful_results = [r for r in results if r["latency"] is not None]
            
            if not successful_results:
                st.error("‚ùå **Performance test failed - TensorFlow model unreachable**")
                st.warning(f"Please ensure your TensorFlow Serving model is running on `{MODEL_HOST}:{MODEL_PORT}`")
                
                # Show sample error for debugging
                error_sample = next((r for r in results if "Connection refused" in str(r.get("response", ""))), None)
                if error_sample:
                    with st.expander("üîç Connection Error Details"):
                        st.code(str(error_sample["response"]))
            else:
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Avg Latency", f"{stats['avg_latency']:.3f}s" if stats['avg_latency'] else "N/A")
                with col2:
                    st.metric("P95 Latency", f"{stats['p95_latency']:.3f}s" if stats['p95_latency'] else "N/A")
                with col3:
                    st.metric("Max Latency", f"{stats['max_latency']:.3f}s" if stats['max_latency'] else "N/A")
                with col4:
                    st.metric("Throughput", f"{stats['throughput']:.2f} req/s")
                
                # Show success/failure breakdown
                failed_requests = len(results) - len(successful_results)
                if failed_requests > 0:
                    st.warning(f"‚ö†Ô∏è {failed_requests} out of {len(results)} requests failed")

def batch_comparison_tab():
    """Batch analysis tab implementation"""
    st.subheader("üìä Batch Analysis")
    
    st.info("Compare performance across different batch sizes")
    
    batch_sizes = st.multiselect(
        "Select batch sizes to compare",
        options=[1, 5, 10, 20, 50, 100],
        default=[1, 10, 50]
    )
    
    num_requests = st.slider("Requests per batch size", 1, 100, 20)
    
    if st.button("üöÄ Run Batch Comparison", use_container_width=True) and batch_sizes:
        comparison_results = []
        progress_bar = st.progress(0)
        
        # Track if any tests succeed
        any_success = False
        
        for i, batch_size in enumerate(batch_sizes):
            with st.spinner(f"Testing batch size {batch_size}..."):
                stats, results = throughput_test(batch_size, num_requests, 5)
                comparison_results.append(stats)
                
                # Check if this batch had any successful requests
                if stats['avg_latency'] is not None:
                    any_success = True
                    
                progress_bar.progress((i + 1) / len(batch_sizes))
        
        if not any_success:
            st.error("‚ùå **Batch comparison failed - TensorFlow model unreachable**")
            st.warning(f"Please ensure your TensorFlow Serving model is running on `{MODEL_HOST}:{MODEL_PORT}`")
        else:
            # Display results
            df_results = pd.DataFrame(comparison_results)
            st.dataframe(df_results, use_container_width=True)
            
            # Create visualization
            if len(comparison_results) > 1:
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
                
                # Latency comparison
                ax1.plot(df_results['batch_size'], df_results['avg_latency'], marker='o', label='Avg Latency')
                ax1.plot(df_results['batch_size'], df_results['p95_latency'], marker='s', label='P95 Latency')
                ax1.set_xlabel('Batch Size')
                ax1.set_ylabel('Latency (s)')
                ax1.set_title('Latency vs Batch Size')
                ax1.legend()
                ax1.grid(True, alpha=0.3)
                
                # Throughput comparison
                ax2.plot(df_results['batch_size'], df_results['throughput'], marker='o', color='green')
                ax2.set_xlabel('Batch Size')
                ax2.set_ylabel('Throughput (req/s)')
                ax2.set_title('Throughput vs Batch Size')
                ax2.grid(True, alpha=0.3)
                
                st.pyplot(fig)

def profiling_tab():
    """Advanced profiling tab implementation"""
    st.subheader("üîç Advanced Profiling")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.info("Analyze detailed timing breakdown of inference pipeline")
        num_samples = st.slider("Number of samples", 5, 50, 20)
    
    with col2:
        if st.button("üöÄ Run Profiling", use_container_width=True):
            with st.spinner("Profiling inference pipeline..."):
                stats_df, raw_df = profile_inference_breakdown(num_samples)
                
                if stats_df is not None and raw_df is not None:
                    st.success("‚úÖ Profiling completed!")
                    
                    # Display timing statistics
                    st.subheader("üìä Timing Statistics")
                    st.dataframe(stats_df, use_container_width=True)
                    
                    # Create visualization
                    fig, ax = plt.subplots(figsize=(10, 6))
                    x_pos = np.arange(len(stats_df))
                    bars = ax.bar(x_pos, stats_df['avg_time_ms'], yerr=stats_df['std_time_ms'], capsize=5)
                    ax.set_xlabel('Pipeline Component')
                    ax.set_ylabel('Time (ms)')
                    ax.set_title('Inference Pipeline Timing Breakdown')
                    ax.set_xticks(x_pos)
                    ax.set_xticklabels(stats_df['component'], rotation=45)
                    ax.grid(True, alpha=0.3)
                    
                    # Add value labels on bars
                    for i, bar in enumerate(bars):
                        height = bar.get_height()
                        ax.text(bar.get_x() + bar.get_width()/2., height + stats_df['std_time_ms'].iloc[i],
                               f'{height:.2f}ms', ha='center', va='bottom')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # Show raw data
                    with st.expander("üìã Raw Timing Data"):
                        st.dataframe(raw_df, use_container_width=True)
                else:
                    st.error("‚ùå **Profiling failed - TensorFlow model unreachable**")
                    st.warning(f"Please ensure your TensorFlow Serving model is running on `{MODEL_HOST}:{MODEL_PORT}`")

def reports_tab():
    """Reports tab implementation"""
    st.subheader("üì• Reports")
    
    col1, col2 = st.columns(2)
    with col1:
        st.info("Export test results and generate reports")
        report_type = st.selectbox(
            "Select report type",
            ["Performance Summary", "Batch Analysis", "Profiling Report", "Full Test Suite"]
        )
    
    with col2:
        st.markdown("### Quick Actions")
        if st.button("üìä Generate Sample Report", use_container_width=True):
            # Generate a sample report
            sample_data = {
                'Metric': ['Avg Latency', 'P95 Latency', 'Throughput', 'Success Rate'],
                'Value': ['0.125s', '0.234s', '45.2 req/s', '99.8%'],
                'Status': ['‚úÖ Good', '‚ö†Ô∏è Fair', '‚úÖ Good', '‚úÖ Excellent']
            }
            
            st.subheader(f"üìã {report_type}")
            st.dataframe(pd.DataFrame(sample_data), use_container_width=True)
            
            # Create download link for CSV
            csv = pd.DataFrame(sample_data).to_csv(index=False)
            st.download_button(
                label="üíæ Download CSV",
                data=csv,
                file_name=f"rainfall_model_{report_type.lower().replace(' ', '_')}.csv",
                mime="text/csv"
            )

# --- Main Dashboard ---
def main_dashboard():
    """Main dashboard with tabs"""
    col1, col2 = st.columns([3, 1])
    with col1:
        st.title("üîç Rainfall Model Testing Dashboard")
        st.markdown(f"Test and benchmark the deployed TensorFlow model at `{MODEL_HOST}:{MODEL_PORT}`")
    with col2:
        st.markdown(f"**Welcome, {st.session_state.get('username', 'User')}!**")
        if st.button("üö™ Logout", use_container_width=True):
            logout()

    # Configuration section
    with st.expander("‚öôÔ∏è Model Configuration"):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.code(f"HOST: {MODEL_HOST}")
        with col2:
            st.code(f"PORT: {MODEL_PORT}")
        with col3:
            st.code(f"URL: {MODEL_URL}")
        
        st.info("üí° To change host/port, set environment variables: `MODEL_HOST` and `MODEL_PORT`")

    # Model status check
    with st.container():
        col1, col2, col3 = st.columns(3)
        with col1:
            try:
                test_result = run_inference(batch_payload(1))
                if test_result["status_code"] == 200:
                    st.success("üü¢ Model Online")
                elif test_result["status_code"] == "Error":
                    st.error("üî¥ TensorFlow model URL is unreachable/unavailable")
                else:
                    st.error(f"üî¥ Model Error (HTTP {test_result['status_code']})")
            except Exception as e:
                st.error("üî¥ TensorFlow model URL is unreachable/unavailable")
        
        with col2:
            st.info(f"üåê Host: {MODEL_HOST}:{MODEL_PORT}")
        with col3:
            st.info(f"üïí Last Check: {time.strftime('%H:%M:%S')}")

    # Main tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üß™ Smoke Test", 
        "‚ö° Performance", 
        "üìä Batch Analysis", 
        "üîç Advanced Profiling", 
        "üì• Reports"
    ])

    with tab1:
        smoke_test_tab()
    with tab2:
        performance_test_tab()
    with tab3:
        batch_comparison_tab()
    with tab4:
        profiling_tab()
    with tab5:
        reports_tab()

    st.markdown("---")
    st.caption("Built for TensorFlow Serving Inference Analysis üöÄ")

# --- Main ---
def main():
    """Main application entry point"""
    # Initialize session state
    if "authenticated" not in st.session_state:
        st.session_state["authenticated"] = False
    if "username" not in st.session_state:
        st.session_state["username"] = None
    
    # Show login or dashboard based on authentication
    if not st.session_state["authenticated"]:
        login_form()
    else:
        main_dashboard()

if __name__ == "__main__":
    # Configure Streamlit page
    st.set_page_config(
        page_title="Rainfall Model Testing Dashboard",
        page_icon="üîç",
        layout="wide",
        initial_sidebar_state="collapsed"
    )
    
    # Custom CSS styling
    st.markdown("""
    <style>
    .main > div { padding-top: 1rem; }
    .stButton > button { width: 100%; }
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p { font-size: 16px; }
    .stTabs [data-baseweb="tab"] { height: 50px; }
    .stMetric > label { font-size: 14px; }
    .stSuccess, .stError, .stWarning, .stInfo { margin-top: 1rem; margin-bottom: 1rem; }
    </style>
    """, unsafe_allow_html=True)
    
    main()
